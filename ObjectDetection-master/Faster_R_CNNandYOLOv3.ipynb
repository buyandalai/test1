{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUlkr_F6dt6k"
      },
      "source": [
        "Problem 1: Please refer to the README to run the above implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2rs_i1crdt6t",
        "outputId": "ca78f386-b8d8-45be-d0a5-a0ac4f9670f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.7 in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from h5py==2.10.0) (1.26.4)\n",
            "Requirement already satisfied: six in c:\\users\\work\\appdata\\roaming\\python\\python311\\site-packages (from h5py==2.10.0) (1.16.0)\n",
            "Building wheels for collected packages: h5py\n",
            "  Building wheel for h5py (setup.py): started\n",
            "  Building wheel for h5py (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for h5py\n",
            "Failed to build h5py\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [74 lines of output]\n",
            "      C:\\Users\\Work\\AppData\\Roaming\\Python\\Python311\\site-packages\\setuptools\\__init__.py:81: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Requirements should be satisfied by a PEP 517 installer.\n",
            "              If you are using pip, you can try `pip install --use-pep517`.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        dist.fetch_build_eggs(dist.setup_requires)\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-311\n",
            "      creating build\\lib.win-amd64-cpython-311\\h5py\n",
            "      copying h5py\\h5py_warnings.py -> build\\lib.win-amd64-cpython-311\\h5py\n",
            "      copying h5py\\highlevel.py -> build\\lib.win-amd64-cpython-311\\h5py\n",
            "      copying h5py\\ipy_completer.py -> build\\lib.win-amd64-cpython-311\\h5py\n",
            "      copying h5py\\version.py -> build\\lib.win-amd64-cpython-311\\h5py\n",
            "      copying h5py\\__init__.py -> build\\lib.win-amd64-cpython-311\\h5py\n",
            "      creating build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\attrs.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\base.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\compat.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\dataset.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\datatype.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\dims.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\files.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\filters.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\group.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\selections.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\selections2.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\vds.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      copying h5py\\_hl\\__init__.py -> build\\lib.win-amd64-cpython-311\\h5py\\_hl\n",
            "      creating build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\common.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_attribute_create.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_attrs.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_attrs_data.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_base.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_completions.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dataset.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dataset_getitem.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dataset_swmr.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_datatype.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_deprecation.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dimension_scales.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dims_dimensionproxy.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_dtype.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_file.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_file2.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_file_image.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_filters.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_group.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5d_direct_chunk.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5f.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5p.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5pl.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_h5t.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_objects.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_selections.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_slicing.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\test_threads.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      copying h5py\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\n",
            "      creating build\\lib.win-amd64-cpython-311\\h5py\\tests\\test_vds\n",
            "      copying h5py\\tests\\test_vds\\test_highlevel_vds.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\\test_vds\n",
            "      copying h5py\\tests\\test_vds\\test_lowlevel_vds.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\\test_vds\n",
            "      copying h5py\\tests\\test_vds\\test_virtual_source.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\\test_vds\n",
            "      copying h5py\\tests\\test_vds\\__init__.py -> build\\lib.win-amd64-cpython-311\\h5py\\tests\\test_vds\n",
            "      running build_ext\n",
            "      Loading library to get version: hdf5.dll\n",
            "      error: Could not find module 'hdf5.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for h5py\n",
            "ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "! pip install h5py==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Omc5-YNdt6t",
        "outputId": "716181ca-8060-4e89-b93e-d472f3653ce8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python: can't open file 'c:\\\\Users\\\\Work\\\\Documents\\\\GitHub\\\\test1\\\\predict.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python predict.py -i ./kaggle_simpson_testset -c .\\train_20230420-231810_config.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWAhIZG8dt6u"
      },
      "source": [
        "Problem 2: Code reading\n",
        "\n",
        "- RPN: It generates candidate regions of interest (ROIs) based on the input image. RPN speeds up the calculation of area proposals, which has been a bottleneck in calculation speed.\n",
        "\n",
        "- RoI pooling layer: A convolution layer is shared between RPN and R-CNN in order to improve the efficiency of calculation cost. Therefore, the RoI pooling layer outputs the proposed areas of various sizes in a fixed size.\n",
        "\n",
        "- IOU: The IOU, which is the overlap between the ground truth and the anchor, is calculated to determine the proposed area.\n",
        "\n",
        "    From the train.py\n",
        "\n",
        "row 71th: model_rpn, model_classifier, model_all = faster_rcnn.get_model(C, classes_count)\n",
        "\n",
        "load RPN model_rpn using Get_model from faster_rcnn.py\n",
        "\n",
        "row 102th: R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_data_format(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
        "\n",
        "RPN model training, estimation and sending to Roi pooling\n",
        "\n",
        "The threshold is set to 0.7 and iou is calculated. The maximum number of proposal area boxes is set to 300."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFBk0Acpdt6u"
      },
      "source": [
        "Problem 3: Estimation by learned weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NjRPtZgqdt6u",
        "outputId": "fc3272ea-e1e3-4bf4-f0d4-37da50e1608a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python: can't open file 'c:\\\\Users\\\\Work\\\\Documents\\\\GitHub\\\\test1\\\\yolo\\\\convert.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python ./yolo/convert.py ./yolo/yolov3.cfg ./yolo/yolov3.weights ./yolo/model_data/yolo_weights.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pPzZQ_0dt6v"
      },
      "source": [
        "Problem 4: Create a file for learning.\n",
        "Learn new data (Simpsons dataset). Read the Training section of the README.md and create the necessary files to train the Simpsons dataset.\n",
        "\n",
        "The format of the annotation file is different from the implementation of Problem 1, so it needs to be converted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wrn4lihZdt6v",
        "outputId": "5bbdfa8d-4f48-44d0-cd02-360941706954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   0    1    2    3    4  \\\n",
            "0  simpsons_dataset/abraham_grampa_simpson/pic_00...   57   72   52   72   \n",
            "1  simpsons_dataset/abraham_grampa_simpson/pic_00...   80   31  337  354   \n",
            "2  simpsons_dataset/abraham_grampa_simpson/pic_00...  128   48  285  407   \n",
            "3  simpsons_dataset/abraham_grampa_simpson/pic_00...   72  126  158  275   \n",
            "4  simpsons_dataset/abraham_grampa_simpson/pic_00...  123   61  294  416   \n",
            "\n",
            "                        5  \n",
            "0  abraham_grampa_simpson  \n",
            "1  abraham_grampa_simpson  \n",
            "2  abraham_grampa_simpson  \n",
            "3  abraham_grampa_simpson  \n",
            "4  abraham_grampa_simpson  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "annotation_df = pd.read_csv('annotation.txt',header=None)\n",
        "print(annotation_df.head())\n",
        "\n",
        "le = LabelEncoder()\n",
        "annotation_df.iloc[:,5] = le.fit_transform(annotation_df.iloc[:,5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pqxdo8Djdt6w"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'annotation_rcnn.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m yolo_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m n_sample, n_col \u001b[38;5;241m=\u001b[39m annotation_df\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcnn_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines() \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_sample):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'annotation_rcnn.txt'"
          ]
        }
      ],
      "source": [
        "#Converting annotation file format\n",
        "rcnn_path = 'annotation_rcnn.txt'\n",
        "yolo_path = 'annotation.txt'\n",
        "n_sample, n_col = annotation_df.shape\n",
        "with open(rcnn_path) as f:\n",
        "    lines = f.readlines() \n",
        "    for i in range(n_sample):\n",
        "        line = lines[i]\n",
        "        split_line = line.split(',') \n",
        "        image_path = split_line[0]\n",
        "        split_line[0] = './' + image_path\n",
        "        split_line[-1] = str(annotation_df.iloc[i,5]) + '\\n'  \n",
        "        with open(yolo_path, mode='a') as out_f:\n",
        "            join_line = ','.join(split_line)  \n",
        "            join_line = join_line.replace('.jpg,','.jpg ')  \n",
        "            out_f.write(join_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0jmHX1ddt6x",
        "outputId": "b0ba1453-4744-4fa4-bf52-65b685811625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   0    1    2    3  4\n",
            "0  ./simpsons_dataset/abraham_grampa_simpson/pic_...   72   52   72  0\n",
            "1  ./simpsons_dataset/abraham_grampa_simpson/pic_...   31  337  354  0\n",
            "2  ./simpsons_dataset/abraham_grampa_simpson/pic_...   48  285  407  0\n",
            "3  ./simpsons_dataset/abraham_grampa_simpson/pic_...  126  158  275  0\n",
            "4  ./simpsons_dataset/abraham_grampa_simpson/pic_...   61  294  416  0\n"
          ]
        }
      ],
      "source": [
        "annotation_df = pd.read_csv('annotation.txt',header=None)\n",
        "print(annotation_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4m1N5o5dt6x"
      },
      "source": [
        "Problem 5: Confirmation that learning can be done.\n",
        "Use the file you created in Problem 4 to learn. If it takes a long time to learn in the execution environment, you can just confirm that you can learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8L9RZnrdt6y"
      },
      "outputs": [],
      "source": [
        "!python ./yolo/train.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "OggyEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
