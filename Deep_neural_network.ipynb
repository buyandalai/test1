{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0xLwcrxHZVys"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC_CsyUESTvP"
      },
      "source": [
        "[Problem 1] Classification of fully connected layers\n",
        "Please classify the fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W3yw-bAdarYN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer,activation):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.activation = activation\n",
        "        # Initialization\n",
        "        # Initialize self.W and self.B using the #initialr method\n",
        "\n",
        "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = self.initializer.B(self.n_nodes2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"     \n",
        "        self.X = X\n",
        "        self.A = np.dot(self.X, self.W) + self.B\n",
        "        A = self.activation.forward(self.A)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        dA = self.activation.backward(dA)\n",
        "        self.dB = np.mean(dA, axis = 0)\n",
        "        self.dW = np.dot(self.X.T, dA)/len(self.X)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "         \n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IMJHx7dU1As"
      },
      "source": [
        "[Problem 2] Classification of initialization method\n",
        "Classify the code that does the initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FdFZ2VrfawSt"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWX4-VpEU6DQ"
      },
      "source": [
        "[Problem 3] Classification of optimization methods Do a class of optimization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rQPFArR-U8x7"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "        \"\"\"\n",
        "        layer.W -= self.lr*layer.dW\n",
        "        layer.B -= self.lr*layer.dB\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQq2NTHU9HS"
      },
      "source": [
        "[Problem 4] Classification of activation function Please classify the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V3xcwbOrVA5f"
      },
      "outputs": [],
      "source": [
        "class softmax():\n",
        "  \"\"\"\n",
        "  Activation function - softmax\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    #print(\"A:\", A)\n",
        "    #print(\"temp:\", np.exp(A - np.max(A)))\n",
        "    #print(\"forward temp:\", np.sum(np.exp(A-np.max(A))))\n",
        "    temp = np.exp(A - np.max(A))/np.sum(np.exp(A-np.max(A)), axis = 1, keepdims= True)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4836mOVBRj"
      },
      "source": [
        "[Problem 5] Create ReLU class Implement ReLU (Rectified Linear Unit), a currently commonly used activation function, as a ReLU class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NiFkU5omVFC-"
      },
      "outputs": [],
      "source": [
        "class ReLU():\n",
        "  \"\"\"\n",
        "  Activation function - relu\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    temp = np.maximum(self.A, 0)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = np.where(self.A>0, dZ, 0)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0g0ptMsybJoN"
      },
      "outputs": [],
      "source": [
        "class tanh():\n",
        "  \"\"\"\n",
        "  Activation function - tangent\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = np.tanh(self.A)\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z**2)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuwSmUexVFbN"
      },
      "source": [
        "[Problem 6] Initial value of weight "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8tzRlePMVJfs"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O29y5dSuXcAS"
      },
      "outputs": [],
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(2/n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEwgrRNNVJy8"
      },
      "source": [
        "[Problem 7] Optimization method "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ISHGPxODVPKg"
      },
      "outputs": [],
      "source": [
        "class Adagrad:\n",
        "  \"\"\"\n",
        "  stochastic gradient descent method\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  lr : learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hW = 0\n",
        "      self.hB = 0\n",
        "  \n",
        "  def update(self,layer):\n",
        "    \"\"\"\n",
        "    Updating the weights and biases of a layer\n",
        "    Parameters\n",
        "    -----------\n",
        "    layer : Instance of the layer before the update\n",
        "    \"\"\"\n",
        "    self.hW += layer.dW * layer.dW\n",
        "    self.hB = layer.dB * layer.dB\n",
        "\n",
        "    layer.W -= self.lr * layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
        "    layer.B -=self.lr * layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2jNL5QDobd_o"
      },
      "outputs": [],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      training data\n",
        "    y : ndarray, shape (n_samples, 1)\n",
        "      Label of training data\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9K9BHm5VPYk"
      },
      "source": [
        "[Problem 8] Class Completion Complete the ScratchDeepNeuralNetrowkClassifier class for training and estimation with any configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GCk71TuRVSr_"
      },
      "outputs": [],
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    A Simple three-layer neural network classifier\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_epoch = 50, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = False):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.verbose = verbose\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.n_batch = n_batch\n",
        "        self.lr = learning_rate\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "    \n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt*np.log(y + delta))\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Label of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Label of validation data\n",
        "        \"\"\"\n",
        "        #self.log_loss = []\n",
        "        # initialize weights\n",
        "        optimizer1 = Adagrad(self.lr)\n",
        "        optimizer2 = Adagrad(self.lr)\n",
        "        optimizer3 = Adagrad(self.lr)\n",
        "\n",
        "        initializer1 = XavierInitializer()\n",
        "        initializer2 = HeInitializer()\n",
        "        initializer3 = SimpleInitializer(self.sigma)\n",
        "\n",
        "        # the loss function for each epoch \n",
        "        \n",
        "\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1,tanh())\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2,tanh())\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3,softmax())\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y , batch_size = self.n_batch)\n",
        "            self.loss = 0\n",
        "\n",
        "            for mini_X_train , mini_y_train in get_mini_batch:\n",
        "                #forward propagation\n",
        "                #1st layer\n",
        "                self.z1 = self.FC1.forward(mini_X_train)\n",
        "                #2nd layer\n",
        "                self.z2 = self.FC2.forward(self.z1)\n",
        "                #3rd layer\n",
        "                self.z3 = self.FC3.forward(self.z2)\n",
        "\n",
        "                # back propagation\n",
        "                #3rd layer\n",
        "                self.dA3 = (self.z3 - mini_y_train)/self.n_batch\n",
        "                #2nd layer\n",
        "                self.dz2 = self.FC3.backward(self.dA3)\n",
        "                #1st layer\n",
        "                self.dz1 = self.FC2.backward(self.dz2)\n",
        "                self.dz0 = self.FC1.backward(self.dz1)\n",
        "\n",
        "                #backpropagation\n",
        "                self.loss += self.loss_function(self.z3, mini_y_train)\n",
        "            \n",
        "            # record loss function for each epoch\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('epoch:{} loss:{}'.format(epoch, self.loss/self.n_batch))        \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            training sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pred:    ndarray, shape (n_samples, 1)\n",
        "            predicted result\n",
        "        \"\"\"\n",
        "\n",
        "        pred_z1 = self.FC1.forward(X)\n",
        "        pred_z2 = self.FC2.forward(pred_z1)\n",
        "        pred = np.argmax(self.FC3.forward(pred_z2), axis=1)\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRp2pPU4VTBd"
      },
      "source": [
        "[Problem 9] Learning and Estimation Create several networks with varying numbers of layers and activation functions. Then learn and estimate the MNIST data and calculate Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "maqGoIMwVXQn",
        "outputId": "3365a770-194a-46e7-a1de-915e81dc9034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train data shape:  (60000, 28, 28)\n",
            "X_test data shape:  (10000, 28, 28)\n",
            "X_train flatten data shape:  (60000, 784)\n",
            "X_test flatten data shape:  (10000, 784)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test flatten data shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (10000, 28, 28)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m)\n\u001b[0;32m     15\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     16\u001b[0m X_train \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\__init__.py:338\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    333\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "print(\"X_train flatten data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test flatten data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('X_train max value:', X_train.max()) # 1.0\n",
        "print('X_train min value:', X_train.min()) # 0.0\n",
        "\n",
        "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
        "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
        "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
        "print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n",
        "\n",
        "clf = ScratchDeepNeuralNetrowkClassifier(n_epoch = 25, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = True)\n",
        "\n",
        "clf.fit(X_train, y_train_one_hot)\n",
        "pred = clf.predict(X_val)\n",
        "print(pred)\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "fig = plt.subplots()\n",
        "plt.title(\"Loss learning graph\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(clf.log_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize\n",
        "y_pred = model_nn.predict(X_test)\n",
        "\n",
        "num = 36 #\n",
        "true_false = y_pred == y_test\n",
        "false_list = np.where(true_false==True)[0].astype(int)\n",
        "X_test = X_test*255.0\n",
        "\n",
        "if false_list.shape[0] < num:\n",
        "    num = false_list.shape[0]\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
        "for i in range(num):\n",
        "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_test[false_list[i]]))\n",
        "    ax.imshow(X_test.reshape(-1,28,28)[false_list[i]], cmap='gray')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyObPVikxLAT3KNKSXkzaOEx",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
